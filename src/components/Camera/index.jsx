// INICIO DEL ARCHIVO COMPLETO Y FINAL: src/components/Camera/index.jsx

import React, { useEffect, useRef, useState, useCallback } from "react";
import * as faceapi from "face-api.js";
import styles from './Camera.module.css';
import { Bar } from 'react-chartjs-2';
import { Chart, BarElement, CategoryScale, LinearScale, Tooltip, Legend } from 'chart.js';
import InfoCard from './InfoCard';

Chart.register(BarElement, CategoryScale, LinearScale, Tooltip, Legend);

const emotionTranslations = { neutral: 'Neutral', happy: 'Feliz', sad: 'Triste', angry: 'Enojado', fearful: 'Asustado', disgusted: 'Disgustado', surprised: 'Sorprendido' };

const getAverageRgb = async (imageSrc) => {
Â  return new Promise((resolve, reject) => {
Â  Â  const img = new Image();
Â  Â  img.crossOrigin = 'Anonymous';
Â  Â  img.src = imageSrc;
Â  Â  img.onload = () => {
Â  Â  Â  const canvas = document.createElement('canvas');
Â  Â  Â  canvas.width = img.width;
Â  Â  Â  canvas.height = img.height;
Â  Â  Â  const ctx = canvas.getContext('2d');
Â  Â  Â  ctx.drawImage(img, 0, 0);
Â  Â  Â  const data = ctx.getImageData(0, 0, img.width, img.height).data;
Â  Â  Â  let r = 0, g = 0, b = 0;
Â  Â  Â  for (let i = 0; i < data.length; i += 4) {
Â  Â  Â  Â  r += data[i];
Â  Â  Â  Â  g += data[i + 1];
Â  Â  Â  Â  b += data[i + 2];
Â  Â  Â  }
Â  Â  Â  const count = data.length / 4;
Â  Â  Â  resolve({ r: r / count, g: g / count, b: b / count });
Â  Â  };
Â  Â  img.onerror = (error) => reject(error);
Â  });
};

// ----------------------------------------------------------------------
//AJUSTAAQUIII: DefiniciÃ³n de la URL del Backend de Render
const RENDER_API_BASE_URL = 'https://facial-recognition-app-frontend.onrender.com/'; 
// ----------------------------------------------------------------------

export default function CameraComponent() {
Â  const videoRef = useRef(null);
Â  const canvasRef = useRef(null);
Â  const animationFrameId = useRef();
Â  const analysisTimerId = useRef();
Â  const countdownIntervalId = useRef();

Â  const emotionHistory = useRef([]);
Â  const ageHistory = useRef([]);
Â  const genderHistory = useRef([]);
Â  const recognitionHistory = useRef([]);
Â  const skinToneHistory = useRef([]);
Â  const faceMatcher = useRef(null);

Â  const [loadingModels, setLoadingModels] = useState(true);
Â  const [loadingMessage, setLoadingMessage] = useState("Despertando a los robots de la IA... ğŸ‘¾");
Â  const [isCameraActive, setIsCameraActive] = useState(true);
Â  const [countdown, setCountdown] = useState(20);
Â  const [finalAnalysisReport, setFinalAnalysisReport] = useState(null);
Â  const [referenceSkinTones, setReferenceSkinTones] = useState(null);

Â  const createFaceMatcher = async () => {
Â  Â  try {
Â  Â  Â  const knownFaces = [
Â  Â  Â  Â  { label: 'Flor', image: 'flor.png' },
Â  Â  Â  Â  { label: 'Hombre de Prueba', image: 'hombre.png' },
Â  Â  Â  Â  { label: 'Marina', image: 'marina.png' },
Â  Â  Â  Â  { label: 'Mujer de Prueba', image: 'mujer.png' }
Â  Â  Â  ];

Â  Â  Â  setLoadingMessage("Aprendiendo rostros de referencia...");

Â  Â  Â  const labeledFaceDescriptors = await Promise.all(
Â  Â  Â  Â  knownFaces.map(async (face) => {
Â  Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  Â  const img = await faceapi.fetchImage(`/known_faces/${face.image}`);
Â  Â  Â  Â  Â  Â  const detections = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor();
Â  Â  Â  Â  Â  Â  if (!detections) {
Â  Â  Â  Â  Â  Â  Â  console.warn(`No se pudo detectar un rostro en la imagen de referencia: ${face.image}`);
Â  Â  Â  Â  Â  Â  Â  return null;
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  return new faceapi.LabeledFaceDescriptors(face.label, [detections.descriptor]);
Â  Â  Â  Â  Â  } catch (e) {
Â  Â  Â  Â  Â  Â  console.error(`Error al cargar o procesar la imagen de referencia ${face.image}:`, e);
Â  Â  Â  Â  Â  Â  return null;
Â  Â  Â  Â  Â  }
Â  Â  Â  Â  })
Â  Â  Â  );

Â  Â  Â  const validDescriptors = labeledFaceDescriptors.filter(descriptor => descriptor !== null);
Â  Â  Â  if (validDescriptors.length === 0) {
Â  Â  Â  Â  console.error("No se pudo cargar ningÃºn descriptor de rostro de referencia vÃ¡lido.");
Â  Â  Â  Â  return;
Â  Â  Â  }

Â  Â  Â  faceMatcher.current = new faceapi.FaceMatcher(validDescriptors, 0.6);
Â  Â  Â  console.log("âœ… FaceMatcher creado con las siguientes identidades:", validDescriptors.map(d => d.label));

Â  Â  } catch (error) {
Â  Â  Â  console.error("ğŸš¨ Error crÃ­tico al crear el FaceMatcher:", error);
Â  Â  }
Â  };

Â  const startVideo = () => {
Â  Â  console.log("ğŸ“¹ Intentando acceder a la cÃ¡mara...");
Â  Â  navigator.mediaDevices
Â  Â  Â  .getUserMedia({ video: {} })
Â  Â  Â  .then((stream) => {
Â  Â  Â  Â  if (videoRef.current) {
Â  Â  Â  Â  Â  console.log("âœ… CÃ¡mara accedida con Ã©xito. Asignando stream al elemento de video.");
Â  Â  Â  Â  Â  videoRef.current.srcObject = stream;
Â  Â  Â  Â  }
Â  Â  Â  })
Â  Â  Â  .catch((err) => console.error("ğŸš¨ ERROR AL ACCEDER A LA CÃMARA:", err));
Â  };

Â  const runFaceDetection = useCallback(async () => {
Â  Â  if (!videoRef.current || videoRef.current.paused || videoRef.current.ended) {
Â  Â  Â  animationFrameId.current = requestAnimationFrame(runFaceDetection);
Â  Â  Â  return;
Â  Â  }

Â  Â  const video = videoRef.current;
Â  Â  if (video.videoWidth === 0) {
Â  Â  Â  animationFrameId.current = requestAnimationFrame(runFaceDetection);
Â  Â  Â  return;
Â  Â  }

Â  Â  const canvas = canvasRef.current;
Â  Â  const displaySize = { width: video.videoWidth, height: video.videoHeight };
Â  Â  faceapi.matchDimensions(canvas, displaySize);

Â  Â  const detections = await faceapi.detectAllFaces(video, new faceapi.SsdMobilenetv1Options({ minConfidence: 0.5 }))
Â  Â  Â  .withFaceLandmarks()
Â  Â  Â  .withAgeAndGender()
Â  Â  Â  .withFaceExpressions()
Â  Â  Â  .withFaceDescriptors();

Â  Â  if (detections.length > 0) {
Â  Â  Â  console.log(`ğŸ˜€ Rostros detectados en este frame: ${detections.length}`);
Â  Â  }

Â  Â  const resizedDetections = faceapi.resizeResults(detections, displaySize);
Â  Â  const ctx = canvas.getContext("2d");
Â  Â  ctx.clearRect(0, 0, canvas.width, canvas.height);

Â  Â  if (detections.length > 0) {
Â  Â  Â  const mainDetection = detections[0];
Â  Â  Â  emotionHistory.current.push(mainDetection.expressions);
Â  Â  Â  ageHistory.current.push(mainDetection.age);
Â  Â  Â  genderHistory.current.push(mainDetection.gender);

Â  Â  Â  if (faceMatcher.current) {
Â  Â  Â  Â  const bestMatch = faceMatcher.current.findBestMatch(mainDetection.descriptor);
Â  Â  Â  Â  recognitionHistory.current.push(bestMatch.label);
Â  Â  Â  }

Â  Â  Â  if (referenceSkinTones) {
Â  Â  Â  Â  const { x, y, width, height } = mainDetection.detection.box;
Â  Â  Â  Â  const tempCanvas = document.createElement('canvas'); tempCanvas.width = width; tempCanvas.height = height;
Â  Â  Â  Â  const tempCtx = tempCanvas.getContext('2d');
Â  Â  Â  Â  tempCtx.drawImage(video, x, y, width, height, 0, 0, width, height);
Â  Â  Â  Â  const data = tempCtx.getImageData(0, 0, width, height).data;
Â  Â  Â  Â  let r = 0, g = 0, b = 0;
Â  Â  Â  Â  for (let i = 0; i < data.length; i += 4) { r += data[i]; g += data[i+1]; b += data[i+2]; }
Â  Â  Â  Â  const count = data.length / 4;
Â  Â  Â  Â  const userColor = { r: r/count, g: g/count, b: b/count };
Â  Â  Â  Â  let closestTone = 'Desconocido', minDistance = Infinity;
Â  Â  Â  Â  referenceSkinTones.forEach(tone => {
Â  Â  Â  Â  Â  const distance = Math.sqrt(Math.pow(userColor.r - tone.color.r, 2) + Math.pow(userColor.g - tone.color.g, 2) + Math.pow(userColor.b - tone.color.b, 2));
Â  Â  Â  Â  Â  if (distance < minDistance) { minDistance = distance; closestTone = tone.name; }
Â  Â  Â  Â  });
Â  Â  Â  Â  skinToneHistory.current.push(closestTone);
Â  Â  Â  }
Â  Â  Â  faceapi.draw.drawDetections(canvas, resizedDetections);
Â  Â  Â  faceapi.draw.drawFaceExpressions(canvas, resizedDetections);
Â  Â  Â  faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
Â  Â  }
Â  Â  animationFrameId.current = requestAnimationFrame(runFaceDetection);
Â  }, [referenceSkinTones]);
Â  
Â  const processFinalAnalysis = useCallback(async () => {
Â  Â  console.log("Procesando anÃ¡lisis final...");
Â  Â  if (emotionHistory.current.length === 0) {
Â  Â  Â  console.log("No se detectaron rostros durante la sesiÃ³n. Mostrando mensaje vacÃ­o.");
Â  Â  Â  setFinalAnalysisReport({ empty: true });
Â  Â  Â  return;
Â  Â  }

Â  Â  const emotionSums = {};
Â  Â  emotionHistory.current.forEach(emotions => { for (const [emotion, value] of Object.entries(emotions)) { if (!emotionSums[emotion]) emotionSums[emotion] = 0; emotionSums[emotion] += value; } });
Â  Â  const averagedEmotions = {};
Â  Â  for (const emotion in emotionSums) { averagedEmotions[emotion] = emotionSums[emotion] / emotionHistory.current.length; }
Â  Â  const translatedAveragedEmotions = {};
Â  Â  for (const englishEmotion in averagedEmotions) { const spanishEmotion = emotionTranslations[englishEmotion] || englishEmotion; translatedAveragedEmotions[spanishEmotion] = averagedEmotions[englishEmotion]; }
Â  Â  const mainEmotion = Object.keys(translatedAveragedEmotions).reduce((a, b) => translatedAveragedEmotions[a] > translatedAveragedEmotions[b] ? a : b);

Â  Â  const averageAge = ageHistory.current.reduce((a, b) => a + b, 0) / ageHistory.current.length;
Â  Â  const genderCount = genderHistory.current.reduce((acc, gender) => { acc[gender] = (acc[gender] || 0) + 1; return acc; }, {});
Â  Â  const detectedGenderByAI = Object.keys(genderCount).reduce((a, b) => genderCount[a] > genderCount[b] ? a : b);

Â  Â  const recognitionCount = recognitionHistory.current.reduce((acc, name) => { acc[name] = (acc[name] || 0) + 1; return acc; }, {});
Â  Â  const detectedIdentity = Object.keys(recognitionCount).length > 0 ? Object.keys(recognitionCount).reduce((a, b) => recognitionCount[a] > recognitionCount[b] ? a : b) : 'unknown';
Â  Â  
Â  Â  const skinToneCount = skinToneHistory.current.reduce((acc, tone) => { acc[tone] = (acc[tone] || 0) + 1; return acc; }, {});
Â  Â  const detectedSkinTone = Object.keys(skinToneCount).length > 0 ? Object.keys(skinToneCount).reduce((a, b) => skinToneCount[a] > skinToneCount[b] ? a : b) : 'No detectado';

Â  Â  let finalGender;
Â  Â  if (['Flor', 'Marina', 'Mujer de Prueba'].includes(detectedIdentity)) {
Â  Â  Â  finalGender = 'Femenino';
Â  Â  } else {
Â  Â  Â  finalGender = detectedGenderByAI === 'male' ? 'Masculino' : 'Femenino';
Â  Â  }

Â  Â  const reportData = {
Â  Â  Â  age: Math.round(averageAge),
Â  Â  Â  gender: finalGender,
Â  Â  Â  mainEmotion: mainEmotion,
Â  Â  Â  allEmotions: translatedAveragedEmotions,
Â  Â  Â  identity: detectedIdentity,
Â  Â  Â  skinTone: detectedSkinTone,
Â  Â  };

Â  Â  console.log("ğŸ“Š Reporte final generado:", reportData);

Â  Â  // ----------------------------------------------------------------------
Â  Â  // ğŸš¨ AJUSTE CRÃTICO: Uso de la URL de Render para enviar la peticiÃ³n al backend
Â  Â  const API_ENDPOINT = `${RENDER_API_BASE_URL}/api/historiales`;

Â  Â  try {
Â  Â  Â  const response = await fetch(API_ENDPOINT, { //Llama a la URL de Render
Â  Â  Â  Â  method: 'POST',
Â  Â  Â  Â  headers: { 'Content-Type': 'application/json' },
Â  Â  Â  Â  body: JSON.stringify(reportData),
Â  Â  Â  });
Â  Â  Â  if (!response.ok) { throw new Error(`Error HTTP: ${response.status} - ${response.statusText}`); }
Â  Â  Â  const responseData = await response.json();
Â  Â  Â  console.log('Respuesta del servidor:', responseData.message);
Â  Â  } catch (error) {
Â  Â  Â  console.error('Error al enviar el historial al backend (Verifique la URL y el servidor):', error);
Â  Â  }
Â  Â  // ----------------------------------------------------------------------
Â  Â  
Â  Â  setFinalAnalysisReport(reportData);
Â  }, []);

Â  const handleReset = () => {
Â  Â  setFinalAnalysisReport(null);
Â  Â  emotionHistory.current = [];
Â  Â  ageHistory.current = [];
Â  Â  genderHistory.current = [];
Â  Â  recognitionHistory.current = [];
Â  Â  skinToneHistory.current = [];
Â  Â  setCountdown(20);
Â  Â  setIsCameraActive(true);
Â  };

Â  useEffect(() => {
Â  Â  const loadInitialModels = async () => {
Â  Â  Â  const MODEL_URL = "/models";
Â  Â  Â  try {
Â  Â  Â  Â  console.log("Iniciando carga de modelos de IA desde:", MODEL_URL);
Â  Â  Â  Â  setLoadingMessage("Cargando modelos de IA...");
Â  Â  Â  Â  await Promise.all([
Â  Â  Â  Â  Â  faceapi.nets.ssdMobilenetv1.loadFromUri(MODEL_URL),
Â  Â  Â  Â  Â  faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
Â  Â  Â  Â  Â  faceapi.nets.ageGenderNet.loadFromUri(MODEL_URL),
Â  Â  Â  Â  Â  faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL),
Â  Â  Â  Â  Â  faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),
Â  Â  Â  Â  ]);
Â  Â  Â  Â  console.log("âœ… Todos los modelos de IA cargados con Ã©xito.");
Â  Â  Â  Â  
Â  Â  Â  Â  await createFaceMatcher();
Â  Â  Â  Â  
Â  Â  Â  Â  console.log("Iniciando calibraciÃ³n de tonos de piel...");
Â  Â  Â  Â  setLoadingMessage("Calibrando tonos de piel de referencia...");
Â  Â  Â  Â  const tones = await Promise.all([
Â  Â  Â  Â  Â  getAverageRgb('/known_faces/marina.png').then(color => ({ name: 'Tes Blanca', color })),
Â  Â  Â  Â  Â  getAverageRgb('/known_faces/negro.png').then(color => ({ name: 'Tes Negra', color })),
Â  Â  Â  Â  Â  getAverageRgb('/known_faces/trigeÃ±a.png').then(color => ({ name: 'TrigueÃ±a', color })),
Â  Â  Â  Â  ]);
Â  Â  Â  Â  setReferenceSkinTones(tones);
Â  Â  Â  Â  console.log("âœ… Tonos de piel calibrados.");
Â  Â  Â  Â  
Â  Â  Â  Â  setLoadingModels(false);
Â  Â  Â  } catch (error) {
Â  Â  Â  Â  console.error("ğŸš¨ ERROR CRÃTICO AL INICIALIZAR LA IA:", error);
Â  Â  Â  Â  setLoadingMessage("Error al cargar los modelos de IA. Revisa la consola.");
Â  Â  Â  }
Â  Â  };
Â  Â  loadInitialModels();
Â  }, []);

Â  useEffect(() => {
Â  Â  if (isCameraActive && !loadingModels) {
Â  Â  Â  startVideo();
Â  Â  Â  const video = videoRef.current;
Â  Â  Â  if (video) {
Â  Â  Â  Â  const handlePlay = () => {
Â  Â  Â  Â  Â  console.log("â–¶ï¸ El video ha comenzado a reproducirse. Iniciando bucle de detecciÃ³n.");
Â  Â  Â  Â  Â  runFaceDetection();
Â  Â  Â  Â  Â  countdownIntervalId.current = setInterval(() => { setCountdown(prev => (prev <= 1 ? 0 : prev - 1)); }, 1000);
Â  Â  Â  Â  Â  analysisTimerId.current = setTimeout(() => {
Â  Â  Â  Â  Â  Â  console.log("âŒ› Tiempo de anÃ¡lisis finalizado. Deteniendo cÃ¡mara y procesando resultados.");
Â  Â  Â  Â  Â  Â  clearInterval(countdownIntervalId.current);
Â  Â  Â  Â  Â  Â  cancelAnimationFrame(animationFrameId.current);
Â  Â  Â  Â  Â  Â  if (videoRef.current && videoRef.current.srcObject) {
Â  Â  Â  Â  Â  Â  Â  videoRef.current.srcObject.getTracks().forEach(track => track.stop());
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  processFinalAnalysis();
Â  Â  Â  Â  Â  Â  setIsCameraActive(false);
Â  Â  Â  Â  Â  }, 20000);
Â  Â  Â  Â  };
Â  Â  Â  Â  video.addEventListener('play', handlePlay);
Â  Â  Â  Â  return () => {
Â  Â  Â  Â  Â  video.removeEventListener('play', handlePlay);
Â  Â  Â  Â  Â  clearInterval(countdownIntervalId.current);
Â  Â  Â  Â  Â  clearTimeout(analysisTimerId.current);
Â  Â  Â  Â  Â  cancelAnimationFrame(animationFrameId.current);
Â  Â  Â  Â  };
Â  Â  Â  }
Â  Â  }
Â  }, [isCameraActive, loadingModels, runFaceDetection, processFinalAnalysis]);

Â  const chartOptions = { indexAxis: 'y', responsive: true, maintainAspectRatio: false, plugins: { legend: { display: false } }, scales: { x: { beginAtZero: true, max: 1, title: { display: true, text: 'Nivel Promedio' } }, y: { title: { display: true, text: 'EmociÃ³n' } } }, };
Â  const chartData = finalAnalysisReport && finalAnalysisReport.allEmotions ? { labels: Object.keys(finalAnalysisReport.allEmotions), datasets: [{ label: 'Nivel de EmociÃ³n', data: Object.values(finalAnalysisReport.allEmotions), backgroundColor: 'rgba(59, 130, 246, 0.8)' }], } : { labels: [], datasets: [] };

Â  return (
Â  Â  <div className={styles.wrapper}>
Â  Â  Â  <video autoPlay loop muted className={styles.backgroundVideo}>
Â  Â  Â  Â  <source src="/fondo.mp4" type="video/mp4" />
Â  Â  Â  </video>
Â  Â  Â  {isCameraActive && !loadingModels && ( <> <h2 className={styles.title}>Recolectando datos...</h2> <p className={styles.description}>SonrÃ­e (o no)... Â¡La IA te estÃ¡ observando! ğŸ§</p> </> )}
Â  Â  Â  {isCameraActive ? (
Â  Â  Â  Â  <>
Â  Â  Â  Â  Â  {loadingModels ? (
Â  Â  Â  Â  Â  Â  <p className={styles.loading}>{loadingMessage}</p>
Â  Â  Â  Â  Â  ) : (
Â  Â  Â  Â  Â  Â  <>
Â  Â  Â  Â  Â  Â  Â  <p className={styles.robotComment}> Â¡Calibrando sensores! Analizando tu vibra... {countdown}s restantes ğŸ‘¾ </p>
Â  Â  Â  Â  Â  Â  Â  <div className={styles.cameraContainer}>
Â  Â  Â  Â  Â  Â  Â  Â  <video ref={videoRef} autoPlay muted playsInline className={styles.video} />
Â  Â  Â  Â  Â  Â  Â  Â  <canvas ref={canvasRef} className={styles.canvas} />
Â  Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  Â  Â  </>
Â  Â  Â  Â  Â  )}
Â  Â  Â  Â  </>
Â  Â  Â  ) : (
Â  Â  Â  Â  <div className={styles.resultsView}>
Â  Â  Â  Â  Â  <h2 className={styles.title}>Â¡Veredicto Final del EmociÃ³metro! ğŸ“œ</h2>
Â  Â  Â  Â  Â  {finalAnalysisReport && !finalAnalysisReport.empty ? (
Â  Â  Â  Â  Â  Â  <>
Â  Â  Â  Â  Â  Â  Â  <div className={styles.infoGrid}>
Â  Â  Â  Â  Â  Â  Â  Â  <InfoCard 
Â  Â  Â  Â  Â  Â  Â  Â  Â  icon="ğŸ†”" 
Â  Â  Â  Â  Â  Â  Â  Â  Â  label="Identidad" 
Â  Â  Â  Â  Â  Â  Â  Â  Â  value={finalAnalysisReport.identity !== 'unknown' ? `Identificado: ${finalAnalysisReport.identity}` : 'No Identificada'} 
Â  Â  Â  Â  Â  Â  Â  Â  Â  highlight={finalAnalysisReport.identity !== 'unknown'} 
Â  Â  Â  Â  Â  Â  Â  Â  />
Â  Â  Â  Â  Â  Â  Â  Â  <InfoCard icon={finalAnalysisReport.gender === 'Masculino' ? 'ğŸ‘¨' : 'ğŸ‘©'} label="Sexo" value={finalAnalysisReport.gender} />
Â  Â  Â  Â  Â  Â  Â  Â  <InfoCard icon="ğŸ‚" label="Edad Estimada" value={`${finalAnalysisReport.age} aÃ±os`} />
Â  Â  Â  Â  Â  Â  Â  Â  <InfoCard icon="ğŸ¨" label="Tono de Piel" value={finalAnalysisReport.skinTone} />
Â  Â  Â  Â  Â  Â  Â  Â  <InfoCard icon="ğŸ˜Š" label="EmociÃ³n Principal" value={finalAnalysisReport.mainEmotion} />
Â  Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  Â  Â  Â  <div className={styles.chartContainer}>
Â  Â  Â  Â  Â  Â  Â  Â  <h3>AnÃ¡lisis Completo de Emociones</h3>
Â  Â  Â  Â  Â  Â  Â  Â  <Bar data={chartData} options={chartOptions} />
Â  Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  Â  Â  </>
Â  Â  Â  Â  Â  ) : (
Â  Â  Â  Â  Â  Â  <div className={styles.placeholderCard}>
Â  Â  Â  Â  Â  Â  Â  <p>Â¡Vaya! Parece que te escondiste muy bien. No vimos ninguna cara para analizar. ğŸ™ˆ</p>
Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  Â  )}
Â  Â  Â  Â  Â  <button onClick={handleReset} className={styles.resetButton}>
Â  Â  Â  Â  Â  Â  Volver a Analizar 
Â  Â  Â  Â  Â  </button>
Â  Â  Â  Â  </div>
Â  Â  Â  )}
Â  Â  </div>
Â  );
}

// FIN DEL ARCHIVO COMPLETO